!pip install torchbnn
## Importing Library
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn import datasets

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import torchbnn as bnn
path='Copy of BNN_Mixed effects_parameters (1)(1).xlsx'
dataset = pd.read_excel(path)
X = dataset.iloc[:, 0:6].values
y = np.log10(dataset.iloc[:,45:].values)
dataset.head()
#Printing shapes
print(dataset.shape)
print(X.shape)
print(y.shape)
y
## Splitting the dataset into train and test 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
X_train = pd.DataFrame(X_train)
X_test = pd.DataFrame(X_test)
y_train = pd.DataFrame(y_train)
y_test = pd.DataFrame(y_test)
#Printing shapes
print(X_train.shape)
print(y_train.shape)
## Scaling the dataset using MinMax Scaler
from sklearn.preprocessing import MinMaxScaler

# Create the MinMaxScaler object
scaler = MinMaxScaler(feature_range=(-1, 1))

# Fit the scaler on the training data and transform it
X = scaler.fit_transform(X)   

# Transform the test data using the same scaler
X_train = scaler.transform(X_train)        
# Transform the test data using the same scaler
X_test = scaler.transform(X_test)         
X_test= pd.DataFrame(X_test)
X_train=pd.DataFrame(X_train)
#Converting X and y into torch tensor
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)
#Printing shapes
print("x_train shape:", X.shape)
print("x_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("x_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)
# Access the underlying NumPy array using the values attribute
X_train_array = X_train.values

# Reshape the array to explicitly specify the shape
X_train_array = X_train_array.reshape(-1, 6)

# Convert the reshaped array into a PyTorch tensor
X_train_tensor = torch.tensor(X_train_array, dtype=torch.float32)

# Access the underlying NumPy array using the values attribute
y_train_array = y_train.values

# Reshape the array to explicitly specify the shape
y_train_array = y_train_array.reshape(-1, 37)

# Convert the reshaped array into a PyTorch tensor
y_train_tensor = torch.tensor(y_train_array, dtype=torch.float32)
## Defining the Model
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import torch.nn.init as init

# Custom prior function for standard Gaussian distribution
def sample_from_standard_gaussian_prior(shape):
    return torch.randn(shape)

# Custom Bayesian Linear layer with a prior function
class CustomBayesLinear(nn.Module):
    def __init__(self, in_features, out_features, prior_fn):
        super(CustomBayesLinear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.prior_fn = prior_fn

        # Weight parameters
        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))
        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features))

        # Bias parameters
        self.bias_mu = nn.Parameter(torch.Tensor(out_features))
        self.bias_rho = nn.Parameter(torch.Tensor(out_features))

        self.reset_parameters()

    def reset_parameters(self):
        # Initialize weights and biases
        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight_mu)
        bound = 1 / math.sqrt(fan_in)

        init.uniform_(self.weight_mu, -bound, bound)
        init.uniform_(self.bias_mu, -bound, bound)

        nn.init.constant_(self.weight_rho, -5)  # Initialize log-variance with a small negative value
        nn.init.constant_(self.bias_rho, -5)

    def forward(self, x):
        # Sample weights and biases from the prior
        weight_epsilon = self.prior_fn(self.weight_mu.size())
        bias_epsilon = self.prior_fn(self.bias_mu.size())

        # Reparameterization trick for sampling
        weight_sample = self.weight_mu + torch.log(1 + torch.exp(self.weight_rho)) * weight_epsilon
        bias_sample = self.bias_mu + torch.log(1 + torch.exp(self.bias_rho)) * bias_epsilon

        # Compute output
        output = F.linear(x, weight_sample, bias_sample)

        return output

# Create your Bayesian neural network using the custom layer
model = nn.Sequential(
    CustomBayesLinear(in_features=6, out_features=12, prior_fn=sample_from_standard_gaussian_prior),
    nn.ELU(),
    CustomBayesLinear(in_features=12, out_features=12, prior_fn=sample_from_standard_gaussian_prior),
    nn.ELU(),
#     CustomBayesLinear(in_features=12, out_features=12, prior_fn=sample_from_standard_gaussian_prior),
#     nn.ELU(),
    CustomBayesLinear(in_features=12, out_features=37, prior_fn=sample_from_standard_gaussian_prior),
)
# Reshape target_data to match the expected output shape of the model
y = y.view(-1, 37)  # Reshaping to [11675, 1]
y.size()
#Defining Loss and optimizer
mse_loss = nn.MSELoss()
kl_loss = bnn.BKLLoss(reduction='mean', last_layer_only=False)
kl_weight = 0.02
optimizer = optim.Adam(model.parameters(), lr=0.001)
kl_loss
## Optimizing the Model
import torch
import torch.nn.functional as F
import torch.optim as optim

def predict_with_uncertainty(model, X_train_tensor, y_train_tensor, num_samples=100, num_steps=3000, kl_weight=0.02, lr=0.001):
    # Define loss function and optimizer
    mse_loss = torch.nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    predictions = []
    for _ in range(num_samples):
        for step in range(num_steps):
            # Forward pass
            pre1 = model(X_train_tensor)

            # Compute loss
            mse = mse_loss(pre1, y_train_tensor)
            kl = kl_loss(model)
            cost = mse + kl_weight * kl

            # Backpropagation and optimization
            optimizer.zero_grad()
            cost.backward()
            optimizer.step()

        predictions.append(pre1.unsqueeze(0))  # Add batch dimension
        
        # Save the state dictionary of the optimized model
        torch.save(model.state_dict(), f'optimized_model_V_H_new_{_}.pth')
#         print(model.state_dict())

    predictions = torch.cat(predictions, dim=0)  # Shape: (num_samples, batch_size, num_classes)
    print(mse)
    print(cost)
    print(kl.item())
    # Calculate mean prediction, aleatoric uncertainty, and epistemic uncertainty
    mean_prediction = torch.mean(predictions, dim=0)
#     median_prediction = torch.median(predictions, dim=0)
    #epistemic_std = torch.std(mean_prediction, dim=0)  # Uncertainty due to model parameter uncertainty
#     epistemic_std = torch.mean(torch.std(predictions, dim=0),dim=0)  # Uncertainty due to noise in the data

    return predictions,mean_prediction
# import torch
# import torch.nn.functional as F
# import torch.optim as optim

# def predict_with_uncertainty(model, X_train_tensor, y_train_tensor, num_samples=3, num_steps=3000, kl_weight=0.5, lr=0.01):
#     # Define optimizer
#     optimizer = optim.Adam(model.parameters(), lr=lr)

#     predictions = []
#     for _ in range(num_samples):
#         for step in range(num_steps):
#             # Forward pass
#             pre1, variance = model(X_train_tensor)

#             # Compute negative log likelihood loss
#             nll_loss = 0.5 * torch.log(variance) + 0.5 * torch.div(torch.pow(y_train_tensor - pre1, 2), variance) + 0.5 * torch.log(torch.tensor(2 * torch.tensor(3.1416)))
#             nll_loss = torch.mean(nll_loss)

#             # Compute KL divergence
#             kl = kl_loss(model)
#             cost = nll_loss + kl_weight * kl

#             # Backpropagation and optimization
#             optimizer.zero_grad()
#             cost.backward()
#             optimizer.step()

#         predictions.append(pre1.unsqueeze(0))  # Add batch dimension
        
    
#         # Save the state dictionary of the optimized model
#         torch.save(model.state_dict(), f'optimized_model_V_H_new_{_}.pth')

#     predictions = torch.cat(predictions, dim=0)  # Shape: (num_samples, batch_size, num_classes)
    
#     # Calculate mean prediction
#     mean_prediction = torch.mean(predictions, dim=0)

#     return predictions, mean_prediction

## Predictions
predictions,mean_prediction = predict_with_uncertainty(model, X_train_tensor, y_train_tensor)

#Printing shape of predictions
predictions.shape
# Load the saved model
# loaded_model = nn.Sequential(
#     CustomBayesLinear(in_features=6, out_features=7, prior_fn=sample_from_standard_gaussian_prior),
#     nn.ReLU(),
#     CustomBayesLinear(in_features=7, out_features=7, prior_fn=sample_from_standard_gaussian_prior),
#     nn.ReLU(),
#     CustomBayesLinear(in_features=7, out_features=7, prior_fn=sample_from_standard_gaussian_prior),
#     nn.ReLU(),
#     CustomBayesLinear(in_features=7, out_features=74, prior_fn=sample_from_standard_gaussian_prior),)
# # loaded_model.load_state_dict(torch.load('optimized_model_0.pth'))
# # loaded_model.eval()

## Plot for new input 
loaded_model = nn.Sequential(
    CustomBayesLinear(in_features=6, out_features=12, prior_fn=sample_from_standard_gaussian_prior),
    nn.ELU(),  
    CustomBayesLinear(in_features=12, out_features=12, prior_fn=sample_from_standard_gaussian_prior),
    nn.ELU(),
#     CustomBayesLinear(in_features=12, out_features=12, prior_fn=sample_from_standard_gaussian_prior),
#     nn.ELU(),
    CustomBayesLinear(in_features=12, out_features=37, prior_fn=sample_from_standard_gaussian_prior),
)
import pandas as pd
import numpy as np

Mw=float(input('Enter Mw: '))
R_epi=float(input('Enter R epi: '))
Vs30= float(input('Enter Vs30: '))
fd=float(input('Enter Event depth: '))
Fm=int(input('Enter Fault mechanism: '))

r=[Mw,np.log10(R_epi),R_epi,np.log10(Vs30),fd,Fm]
l=[]
l.append(r)
l
l=scaler.transform(l)

# Access the underlying NumPy array using the values attribute
X_test_array = l

# Reshape the array to explicitly specify the shape
X_test_array = X_test_array.reshape(-1, 6)

# Convert the reshaped array into a PyTorch tensor
X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32)
# Prediction using loaded Model
X_new_tensor = X_test_tensor
num_step=100
all_predictions=[]
for _ in range(num_step):
    loaded_model.load_state_dict(torch.load(f'optimized_model_V_H_new_{_}.pth'))
    loaded_model.eval()
    output = loaded_model(X_new_tensor)
    all_predictions.append(output.unsqueeze(0))
    inp_predictions = torch.cat(all_predictions, dim=0)
inp_predictions.shape
inp_mean_prediction = torch.mean(inp_predictions, dim=0)
inp_mean_prediction.shape
t = [0,0.01,0.025,0.04,0.05 ,0.07,0.1,0.15,0.2,0.25 ,0.3,0.35,0.4,0.45,0.5,0.6,0.7,0.75,0.8,0.9,1,1.2,1.4,1.6,1.8,2,2.5,3,3.5,4,4.5,5,6,7,8,9,10]

#Plotting a single datapoint
import matplotlib.pyplot as plt
zz = inp_predictions
num_step = 100
for _ in range(num_step): 
    plt.plot(t,np.power(10,zz[_][0].detach().numpy()), label=f'Pre_{_}', color ='skyblue')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.title('Plot of first row of xx vs first row of yy')
    plt.legend()
# plt.plot(X_test_tensor[0][:], label='rec',linewidth = 3,color ='black')
plt.plot(t,np.power(10,inp_mean_prediction[0].detach().numpy()), label='rec',linewidth = 3,color ='red')
plt.xscale('log')
plt.yscale('log')
plt.legend()


# Access the underlying NumPy array using the values attribute
X_test_array = X_test.values

# Reshape the array to explicitly specify the shape
X_test_array = X_test_array.reshape(-1, 6)

# Convert the reshaped array into a PyTorch tensor
X_test_tensor = torch.tensor(X_test_array, dtype=torch.float32)
# Access the underlying NumPy array using the values attribute
y_test_array = y_test.values

# Reshape the array to explicitly specify the shape
y_test_array = y_test_array.reshape(-1, 37)

# Convert the reshaped array into a PyTorch tensor
y_test_tensor = torch.tensor(y_test_array, dtype=torch.float32)
# Prediction using loaded Model
X_new_tensor = X
num_step=100
all_predictions=[]
for _ in range(num_step):
    loaded_model.load_state_dict(torch.load(f'optimized_model_V_H_new_{_}.pth'))
    loaded_model.eval()
    output = loaded_model(X_new_tensor)
    all_predictions.append(output.unsqueeze(0))
    predictions_new = torch.cat(all_predictions, dim=0)
 predictions_new.shape
import scipy.io

# Assuming predictions_new is your PyTorch tensor of shape (100, 11675, 37)

# Convert PyTorch tensor to a NumPy array
predictions_numpy = predictions_new.cpu().detach().numpy()

# Save NumPy array to MAT file
scipy.io.savemat('predictions_BNN_V_H.mat', {'predictions': predictions_numpy})

print("MAT file saved successfully.")
new_mean_prediction = torch.mean(predictions_new, dim=0)
# new_median_prediction = torch.median(predictions_new, dim=0)
# new_median_prediction=new_median_prediction.data.numpy()
new_mean_prediction
plt.xlabel(r'$y-rec$')
plt.ylabel(r'$y-pred$')

#plt.scatter(X_test.data.numpy(), y_test.data.numpy(), color='k', s=2)
#serial =  range(0,37)
serial = 35
# XX = y_train_tensor[:, serial].data.numpy()
XX = y_test_tensor[:, serial].data.numpy()
YY = new_mean_prediction[:, serial]
# YY = mean_prediction[:, serial]

plt.scatter(XX, YY.detach().numpy(), color='r', s=10)

#plt.scatter( y_test,y_predict.data.numpy(), color='r', s=10)

min_val = min(np.min(XX), np.min(YY.detach().numpy()))
max_val = max(np.max(XX), np.max(YY.detach().numpy()))
plt.plot([min_val, max_val], [min_val, max_val], color='black', linestyle='--', label='Perfect Prediction')

plt.legend()

plt.show()

YY.shape
serial = range(0,37)
# xx = y_train_tensor[:, serial].data.numpy()
xx = y_test_tensor[:, serial].data.numpy()
yy = new_mean_prediction[:, serial].detach().numpy()
# yy = mean_prediction[:, serial]
# yy=yy.detach().numpy()


#Calculating R2 square for each time period
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

xx_transposed = np.transpose(xx)
yy_transposed = np.transpose(yy)

r_squared_values = []
for i in range(len(xx_transposed)):
    r_squared = r2_score(xx_transposed[i], yy_transposed[i])
    r_squared_values.append(r_squared)

rounded_values = [round(value, 2) for value in r_squared_values]
print(rounded_values)
zz.shape
#Plotting a single datapoint
import matplotlib.pyplot as plt
zz = predictions_new
num_step=25
for _ in range(num_step): 
    plt.plot(zz[_][20].detach().numpy(), label=f'Pre_{_}')
    plt.xlabel('Index')
    plt.ylabel('Value')
    plt.title('Plot of first row of xx vs first row of yy')
    plt.legend()
plt.plot(xx[20], label='rec',linewidth = 3,color ='black')
for_plot = pd.read_excel('PSA_vs_period_Vs30.xlsx')
for_plot_1 = for_plot.iloc[:,0:6]
for_plot_1
for_plot_1 = scaler.transform(for_plot_1)   
for_plot_1= pd.DataFrame(for_plot_1)
for_plot_1_array = for_plot_1.values

# Reshape the array to explicitly specify the shape
for_plot_1_array = for_plot_1_array.reshape(-1, 6)

# Convert the reshaped array into a PyTorch tensor
for_plot_1_tensor = torch.tensor(for_plot_1_array, dtype=torch.float32)
X_new_tensor = for_plot_1_tensor
num_step=100
all_predictions=[]
for _ in range(num_step):
    loaded_model.load_state_dict(torch.load(f'optimized_model_V_H_new_{_}.pth'))
    loaded_model.eval()
    output = loaded_model(X_new_tensor)
    all_predictions.append(output.unsqueeze(0))
    predictions_plot_1 = torch.cat(all_predictions, dim=0)
predictions_plot_1.shape
new_mean_prediction_1 = torch.mean(predictions_plot_1, dim=0)
new_mean_prediction_1.shape
new_mean_prediction_1
# Assuming new_mean_prediction is your PyTorch tensor
# Convert PyTorch tensor to a numpy array
numpy_array = new_mean_prediction_1.cpu().detach().numpy()

# Convert numpy array to a pandas DataFrame
df = pd.DataFrame(numpy_array)
df = np.power(10,df)
for_plot_1=pd.DataFrame(scaler.inverse_transform(for_plot_1))
for_plot_1
df
# plt.plot(for_plot_1.iloc[:11,0],df.iloc[:11,8])
# plt.plot(for_plot_1.iloc[11:22,0],df.iloc[11:22,8])
# plt.plot(for_plot_1.iloc[22:33,0],df.iloc[22:33,8])
# plt.plot(for_plot_1.iloc[33:44,0],df.iloc[33:44,8])
# # plt.plot(for_plot_1.iloc[800:1000,2],df.iloc[800:1000,20])
# # plt.plot(for_plot_1.iloc[1000:1200,2],df.iloc[1000:1200,20])
time_period_values = [0, 0.01, 0.025, 0.04, 0.05, 0.07, 0.1, 0.15, 0.2, 0.25, 
                      0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 
                      1, 1.2, 1.4, 1.6, 1.8, 2, 2.5, 3, 3.5, 4, 4.5, 5, 
                      6, 7, 8, 9, 10]

# Create a DataFrame with a single column named "time_period"
tp = pd.DataFrame({"time_period": time_period_values})

# Display the DataFrame
print(tp)

import matplotlib.pyplot as plt

# Assuming for_plot_1 and df are pandas DataFrames
# Plot data with logarithmic scale on both axes
plt.xscale('log')
plt.yscale('log')

# Plot the data
plt.plot(tp.iloc[:,0],df.iloc[5,:],marker='d', linestyle='-', label='Vs30 - 150')
plt.plot(tp.iloc[:,0],df.iloc[6,:], marker='s', linestyle='-', label='Vs30 - 225')
plt.plot(tp.iloc[:,0],df.iloc[7,:], marker='^', linestyle='-', label='Vs30 - 525')
plt.plot(tp.iloc[:,0],df.iloc[8,:], marker='*', linestyle='-', label='Vs30 - 1070')
plt.plot(tp.iloc[:,0],df.iloc[9,:], marker='*', linestyle='-', label='Vs30 - 1500')
# plt.plot(tp.iloc[:,0],df.iloc[19,:], marker='x', linestyle='-', label='40 km')
# plt.plot(for_plot_1.iloc[800:1000,2], df.iloc[800:1000,8], marker='d', linestyle='-', label='Mw-8')
# plt.plot(for_plot_1.iloc[1000:1200,2], df.iloc[1000:1200,8], marker='d', linestyle='-', label='Mw-9')

# Set x-axis limits to display only up to 10^2 (100)
# plt.ylim(40, 150)
# plt.xlim(1, 10)

# Add labels and title
plt.xlabel('Period')
plt.ylabel('PSA')
plt.title('RS_vs_period_Vs30_V_H_2_7.5')

# Add legend
plt.legend()

plt.savefig('RS_vs_period_Vs30_V_H_2_7.5.jpg')

# Show the plot
plt.show()
combined_df = pd.concat([for_plot_1, df], axis=1)
combined_df
# Define the path where you want to save the Excel file
excel_file_path = "RS_vs_period_VS30_V_H_2.xlsx"

# Save DataFrame to Excel
combined_df.to_excel(excel_file_path, index=False)

print("Excel file saved successfully.")
