{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## About Kernel:\n\nThis kernel is all about how one can perform Sentiment Analysis on twitter dataset using Logistic Regression model from scratch. Given a tweet, we will decide if it has a positive sentiment or a negative one. \n\n## About the Twitter dataset\n\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset.\n","metadata":{}},{"cell_type":"code","source":"import nltk                                  # Python library for NLP\nfrom nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt              # visualization library\nimport numpy as np                           # library for scientific computing and matrix operations","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# downloads sample twitter dataset.\nnltk.download('twitter_samples')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom nltk.corpus import twitter_samples \n\n# download the stopwords for the process_tweet function\nnltk.download('stopwords')\n\nimport re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare the data\n* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  \n    ","metadata":{}},{"cell_type":"code","source":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\n# concatenate the lists, 1st part is the positive tweets followed by the negative\ntweets = all_positive_tweets + all_negative_tweets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make a numpy array representing labels of the tweets\nlabels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Train test split: 20% will be in the test set, and 80% in the training set.\n","metadata":{}},{"cell_type":"code","source":"# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Create the numpy array of positive labels and negative labels.","metadata":{}},{"cell_type":"code","source":"# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shape train and test sets\nprint(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocess raw text for Sentiment analysis\n\nData preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\n1. **Tokenizing the string:** To tokenize means to split the strings into individual words without blanks or tabs.\n\n2. **Lowercasing:** In this step, we will also convert each word in the string to lower case.\n\n3. **Removing stop words and punctuation:**\n\nSince we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the re library to perform regular expression operations on our tweet. We'll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '').\n\n4. **Stemming:** Stemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n\nConsider the words: \n * **learn**\n * **learn**ing\n * **learn**ed\n * **learn**t\n \nAll these words are stemmed from its common root **learn**. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, **happi** and **sunni**. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\n * **happ**y\n * **happi**ness\n * **happi**er\n \nWe can see that the prefix **happi** is more commonly used. We cannot choose **happ** because it is the stem of unrelated words like **happen**.\n \nNLTK has different modules for stemming but in this kernel i will be using the PorterStemmer.","metadata":{}},{"cell_type":"code","source":"def process_tweet(tweet):\n   \n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    \n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    \n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    \n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n    \n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    \n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            # tweets_clean.append(word)\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_freqs(tweets, ys):\n    \n    # tweets: a list of tweets\n    # ys: an m x 1 array with the sentiment label of each tweet (either 0 or 1)\n   \n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n    \n    # freqs: a dictionary mapping each (word, sentiment) pair to its frequency\n    \n    return freqs                   ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Table of word counts","metadata":{}},{"cell_type":"code","source":"# create frequency dictionary\nfreqs = build_freqs(tweets, labels)\n\n# check data type\nprint(f'type(freqs) = {type(freqs)}')\n\n# check length of the dictionary\nprint(f'len(freqs) = {len(freqs)}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will select a set of words that we would like to visualize.","metadata":{}},{"cell_type":"code","source":"# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n        'song', 'idea', 'power', 'play', 'magnific']\n\n# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\ndata = []\n\n# loop through our selected words\nfor word in keys:\n    \n    # initialize positive and negative counts\n    pos = 0\n    neg = 0\n    \n    # retrieve number of positive counts\n    if (word, 1) in freqs:\n        pos = freqs[(word, 1)]\n        \n    # retrieve number of negative counts\n    if (word, 0) in freqs:\n        neg = freqs[(word, 0)]\n        \n    # append the word counts to the table\n    data.append([word, pos, neg])\n    \ndata","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can then use a scatter plot to inspect this table visually. Instead of plotting the raw counts, we will plot it in the logarithmic scale to take into account the wide discrepancies between the raw counts (e.g. `:)` has 3568 counts in the positive while only 2 in the negative). The red line marks the boundary between positive and negative areas. Words close to the red line can be classified as neutral. ","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (8, 8))\n\n# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\nx = np.log([x[1] + 1 for x in data])  \n\n# do the same for the negative counts\ny = np.log([x[2] + 1 for x in data]) \n\n# Plot a dot for each pair of words\nax.scatter(x, y)  \n\n# assign axis labels\nplt.xlabel(\"Log Positive count\")\nplt.ylabel(\"Log Negative count\")\n\n# Add the word as the label at the same position as you added the points just before\nfor i in range(0, len(data)):\n    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n\nax.plot([0, 9], [0, 9], color = 'red')  # Plot the red line that divides the 2 areas.\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing tweets and the Logistic Regression model\n","metadata":{}},{"cell_type":"code","source":"# create frequency dictionary\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Process tweet\nThe given function `process_tweet()` tokenizes the tweet into individual words, removes stop words and applies stemming.","metadata":{}},{"cell_type":"code","source":"# test the function below\nprint('This is an example of a positive tweet: \\n', train_x[0])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 1: Logistic regression ","metadata":{}},{"cell_type":"markdown","source":"### Logistic regression\n$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n\n### Cost function and Gradient\n\nThe cost function used for logistic regression is the average of the log loss across all training examples:\n\n$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$\n* $m$ is the number of training examples\n* $y^{(i)}$ is the actual label of the i-th training example.\n* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n\nThe loss function for a single training example is\n$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n\n\n* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n\n### Update the weights\n\nTo update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \nThe gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n\n$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j \\tag{5}$$\n* 'i' is the index across all 'm' training examples.\n* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n\n* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n","metadata":{}},{"cell_type":"code","source":"def sigmoid(z): \n\n    h = 1/(1+np.exp(-z))\n   \n    return h","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gradientDescent(x, y, theta, alpha, num_iters):\n   \n    m = x.shape[0]\n    \n    for i in range(0, num_iters):\n        \n        # get z, the dot product of x and theta\n        z = np.dot(x,theta)\n        \n        # get the sigmoid of z\n        h = sigmoid(z)\n        \n        # calculate the cost function\n        J = -(np.dot(y.T,np.log(h))+np.dot((1-y).T,np.log(1-h)))/m\n\n        # update the weights theta\n        theta = theta - alpha*(np.dot(x.T,h-y))/m\n        \n    J = float(J)\n    return J, theta","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 2: Extracting the features\n\n* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n    * The first feature is the number of positive words in a tweet.\n    * The second feature is the number of negative words in a tweet. \n* Then train your logistic regression classifier on these features.\n* Test the classifier on a validation set. \n","metadata":{}},{"cell_type":"code","source":"def extract_features(tweet, freqs):\n    \n    # process_tweet tokenizes, stems, and removes stopwords\n    word_l = process_tweet(tweet)\n    \n    # 3 elements in the form of a 1 x 3 vector\n    x = np.zeros((1, 3)) \n    \n    #bias term is set to 1\n    x[0,0] = 1 \n       \n    # loop through each word in the list of words\n    for word in word_l:\n         \n        if (word,1.0) in freqs:\n            # increment the word count for the positive label 1\n            x[0,1] += freqs[(word,1.0)]\n        if(word,0.0) in freqs:\n            # increment the word count for the negative label 0\n            x[0,2] += freqs[(word,0.0)]\n        \n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 3: Training Your Model\n\nTo train the model:\n* Stack the features for all training examples into a matrix `X`. \n* Call `gradientDescent`, which we've implemented above.","metadata":{}},{"cell_type":"code","source":"# collect the features 'x' and stack them into a matrix 'X'\nX = np.zeros((len(train_x), 3))\nfor i in range(len(train_x)):\n    X[i, :]= extract_features(train_x[i], freqs)\n\n# training labels corresponding to X\nY = train_y\n\nprint(X.shape)\nprint(Y.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply gradient descent\nJ, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n\nprint(f\"The cost after training is {J:.6f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 4: Visualizing tweets ","metadata":{}},{"cell_type":"code","source":"# Plot the samples using columns 1 and 2 of the matrix\nfig, ax = plt.subplots(figsize = (10, 8))\n\ncolors = ['red', 'green']\n\n# Color based on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the plot, it is evident that the features that we have chosen to represent tweets as numerical vectors allow an almost perfect separation between positive and negative tweets. So we can expect a very high accuracy for this model! \n\n## Plot the model alongside the data\n\nWe will draw a gray line to show the cutoff between the positive and negative regions. In other words, the gray line marks the line where $$ z = \\theta * x = 0.$$\nTo draw this line, we have to solve the above equation in terms of one of the independent variables.\n\n$$ z = \\theta * x = 0$$\n$$ x = [1, pos, neg] $$\n$$ z(\\theta, x) = \\theta_0+ \\theta_1 * pos + \\theta_2 * neg = 0 $$\n$$ neg = (-\\theta_0 - \\theta_1 * pos) / \\theta_2 $$","metadata":{}},{"cell_type":"code","source":"# Equation for the separation plane\ndef neg(theta, pos):\n    return (-theta[0] - pos * theta[1]) / theta[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 8))\n\ncolors = ['red', 'green']\n\n# Color base on the sentiment Y\nax.scatter(X[:,1], X[:,2], c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\nplt.xlabel(\"Positive\")\nplt.ylabel(\"Negative\")\n\n# Now lets represent the logistic regression model in this chart. \nmaxpos = np.max(X[:,1])           # max value in x-axis\n\n# Plot a gray line that divides the 2 areas.\nax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 5: Test your logistic regression","metadata":{}},{"cell_type":"code","source":"def predict_tweet(tweet, freqs, theta):\n\n    # extract the features of the tweet and store it into x\n    x = extract_features(tweet, freqs)\n    \n    # make the prediction using x and theta\n    y_pred = sigmoid(np.dot(x,theta))\n    \n    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check your own sentiment\nmy_tweet = 'I love machine learning :)'\ny_pred_temp = predict_tweet(my_tweet, freqs, theta)\nprint(y_pred_temp)\n\nif y_pred_temp > 0.5:\n    print('Positive sentiment')\nelse: \n    print('Negative sentiment')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_logistic_regression(test_x, test_y, freqs, theta):\n    \"\"\" \n    test_x: a list of tweets\n    test_y: (m, 1) vector with the corresponding labels for the list of tweets\n    \"\"\"\n    \n    y_hat = []\n    \n    for tweet in test_x:\n        # get the label prediction for the tweet\n        y_pred = predict_tweet(tweet, freqs, theta)\n        \n        if y_pred > 0.5:\n            y_hat.append(1.0)\n        else:\n            y_hat.append(0.0)\n\n    accuracy = np.sum(np.squeeze(test_y) == np.squeeze(np.asarray(y_hat)))/len(test_y)\n\n    return accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_accuracy = test_logistic_regression(test_x, test_y, freqs, theta)\nprint(f\"Logistic regression model's accuracy = {test_accuracy:.4f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Part 6: Error Analysis\n\nIn this part you will see some tweets that your model misclassified. Why do you think the misclassifications happened? Specifically what kind of tweets does your model misclassify?","metadata":{}},{"cell_type":"code","source":"print('Label Predicted Tweet')\nfor x,y in zip(test_x,test_y):\n    y_hat = predict_tweet(x, freqs, theta)\n\n    if np.abs(y - (y_hat > 0.5)) > 0:\n        print('THE TWEET IS:', x)\n        print('THE PROCESSED TWEET IS:', process_tweet(x))\n        print('%d\\t%0.8f\\t%s' % (y, y_hat, ' '.join(process_tweet(x)).encode('ascii', 'ignore')))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If you really like this kernel then please do not forget to upvote it... üòä‚úåüèª","metadata":{}}]}